# -*- coding: utf-8 -*-
"""poem-generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UXL7K1u-2N0owADNFA6WyUdJra0sqybl
"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip ./glove.6B.zip

from sklearn.preprocessing import OneHotEncoder
import keras
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

#you could find the dataset in kaggle
latent_dim=25
emb_dim=50
max_vocabs=3000
df=pd.read_csv("/content/robert_frost_collection.csv")
data=df["Content"]
data=data.dropna()

text=[]
for line in data:
  
  temp="<sos> "+line.lower()+" <eos>"
  text.append(temp)
print(text)

tokenizer=keras.preprocessing.text.Tokenizer(num_words=max_vocabs,filters="   ")
tokenizer.fit_on_texts(text)
text=tokenizer.texts_to_sequences(text)
word2index=tokenizer.word_index
max_sequance_len=max(len(s) for s in text)
min_sequance=min(len(s) for s in text)

input_text=[]
target_text=[]
for line in text:
  for i in range(0,max_sequance_len):
    input_text.append(line[i:(min_sequance-5)+i])
    temp=line[(min_sequance-5)+i:(min_sequance-5)+i+1]
    target_text.append(temp)
    if temp==word2index["<eos>"]:
      break

input_text=input_text[:10000]
target_text=target_text[:10000]
max_sequance_len=max(len(s) for s in input_text)
input_text=keras.preprocessing.sequence.pad_sequences(input_text,maxlen=max_sequance_len,padding="post")
target_text=keras.preprocessing.sequence.pad_sequences(target_text,maxlen=max_sequance_len,padding="post")

# from tensorflow.keras.utils import to_categorical
# target=to_categorical(target_text)
num_word=min(len(word2index)+1,max_vocabs)
target=np.zeros((len(target_text),max_sequance_len,num_word))
for i in range(len(target_text)):
  for j in range(len(target_text[i])):
    if (input_text[i][j]-1)>-1:
     target[i,j,(input_text[i][j]-1)]=1

word2vec={}
f=open("./glove.6B."+str(emb_dim)+"d.txt")    
for line in f:
        temp=line.split()
        word=temp[0]
        vec=np.asarray(temp[1:],dtype="float32")
        word2vec[word]=vec

embedding_matrix=np.zeros((num_word,emb_dim))
for word,i in word2index.items():
  if i<max_vocabs:
    embedding_vec=word2vec.get(word)
    if  embedding_vec is not None:
        embedding_matrix[i]=embedding_vec

embedding_layer=keras.layers.Embedding(num_word,emb_dim,weights=[embedding_matrix]
                                        ,input_length=max_sequance_len)

input_=keras.layers.Input(shape=(max_sequance_len,))
initial_h=keras.layers.Input(shape=(latent_dim,))
initial_c=keras.layers.Input(shape=(latent_dim,))
x=embedding_layer(input_)
lstm=keras.layers.LSTM(latent_dim,return_sequences=True, return_state=True)
x,_,_=lstm(x,initial_state=[initial_h,initial_c])
dense=keras.layers.Dense(num_word,activation="softmax")
y=dense(x)
model=keras.Model([input_,initial_h,initial_c],y)

model.compile(optimizer="adam",loss=keras.losses.CategoricalCrossentropy(),metrics=["accuracy"])

z=np.zeros((len(input_text),latent_dim))
# print("input text shape: "+str(len(word2index))," output shape: "+str(target.shape)," z shape: "+str(z.shape))
model.fit([input_text,z,z]
          ,target,batch_size=32,epochs=10,validation_split=0.2)

input2_=keras.layers.Input(shape=(1,))
x=embedding_layer(input2_)
x,h,c=lstm(x,initial_state=[initial_h,initial_c])
y2=dense(x)
model=keras.Model([input2_,initial_h,initial_c],[y2,h,c])

gen_sen=""
h=np.zeros((1,latent_dim))
c=np.zeros((1,latent_dim))
_input=np.asarray([[word2index["<sos>"]]])
size=max_sequance_len
for i in range(size):
  _input,h,c=model.predict([_input,h,c])
  prop=_input[0,0]
  prop[0]=0
  prop/=prop.sum()
  idx=np.random.choice(len(prop),p=prop)
  word=tokenizer.index_word[idx]  
  _input=np.asarray([[idx]])
  if size-1==i:
    print("done before finding eos")
    print(gen_sen)
  if word=="<eos>":
    
    print(gen_sen)
    break
  else:
    if gen_sen=="":
      gen_sen+=word
    else:
      gen_sen+=" "+word

